{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724cd313",
   "metadata": {},
   "source": [
    "# CSV â†’ Parquet (GCS)\n",
    "Minimal pipeline from bronze to processed partitioned Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036236ff",
   "metadata": {},
   "source": [
    "## Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63e38f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/02 06:52:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Hadoop user for local FS access\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"spark\"\n",
    "\n",
    "# Stop existing SparkSession\n",
    "try:\n",
    "  spark.stop()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "  .appName(\"csv_parquet\")\n",
    "  .master(\"spark://spark-master:7077\")\n",
    "\n",
    "  # Ivy cache directory\n",
    "  .config(\"spark.jars.ivy\", \"/home/spark/.ivy2\")\n",
    "\n",
    "  # GCS connector settings\n",
    "  .config(\n",
    "      \"spark.hadoop.fs.gs.impl\",\n",
    "      \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n",
    "  )\n",
    "  .config(\n",
    "      \"spark.hadoop.fs.AbstractFileSystem.gs.impl\",\n",
    "      \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    "  )\n",
    "\n",
    "  # GCS service account auth\n",
    "  .config(\n",
    "      \"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\"\n",
    "  )\n",
    "  .config(\n",
    "      \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",\n",
    "      \"/cred/clickstream-sa.json\"\n",
    "  )\n",
    "\n",
    "  .getOrCreate()\n",
    ")\n",
    "\n",
    "# Log level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72840587",
   "metadata": {},
   "source": [
    "## Paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f0e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  gs://clickstream-pipeline-484705-clickstream-data/raw/2019-Oct.csv, gs://clickstream-pipeline-484705-clickstream-data/raw/2019-Nov.csv\n",
      "Output (invalid): gs://clickstream-pipeline-484705-clickstream-data/raw/invalid\n",
      "Output (processed):  gs://clickstream-pipeline-484705-clickstream-data/processed/clickstreams\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, regexp_replace, monotonically_increasing_id\n",
    "from pyspark.sql.functions import when, lit, concat_ws, array, to_date, date_format\n",
    "\n",
    "# GCS paths\n",
    "GCS_BUCKET = \"clickstream-pipeline-484705-clickstream-data\"\n",
    "INPUT_PATHS = [\n",
    "  f\"gs://{GCS_BUCKET}/raw/2019-Oct.csv\",\n",
    "  f\"gs://{GCS_BUCKET}/raw/2019-Nov.csv\",\n",
    "]\n",
    "\n",
    "OUTPUT_PATHS = {\n",
    "  \"bronze_invalid\": f\"gs://{GCS_BUCKET}/raw/invalid\",\n",
    "  \"processed\": f\"gs://{GCS_BUCKET}/processed/clickstream\",\n",
    "}\n",
    "\n",
    "print(f\"Input:  {', '.join(INPUT_PATHS)}\")\n",
    "print(f\"Output (invalid): {OUTPUT_PATHS['bronze_invalid']}\")\n",
    "print(f\"Output (processed):  {OUTPUT_PATHS['processed']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47464b",
   "metadata": {},
   "source": [
    "## Bronze schema (all strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f5eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "bronze_schema = types.StructType([                                                                                        \n",
    "  types.StructField(\"event_time\", types.StringType(), True),                                                            \n",
    "  types.StructField(\"event_type\", types.StringType(), True),                                                            \n",
    "  types.StructField(\"product_id\", types.StringType(), True),                                                            \n",
    "  types.StructField(\"category_id\", types.StringType(), True),                                                           \n",
    "  types.StructField(\"category_code\", types.StringType(), True),                                                         \n",
    "  types.StructField(\"brand\", types.StringType(), True),                                                                 \n",
    "  types.StructField(\"price\", types.StringType(), True),                                                                 \n",
    "  types.StructField(\"user_id\", types.StringType(), True),                                                               \n",
    "  types.StructField(\"user_session\", types.StringType(), True),                                                          \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b772ff",
   "metadata": {},
   "source": [
    "## Load bronze data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9aed3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from GCS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(109 + 1) / 110]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read complete: 109,950,743 records\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Loading data from GCS...\")\n",
    "df_bronze = (\n",
    "  spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(bronze_schema)\n",
    "  .csv(INPUT_PATHS)\n",
    ")\n",
    "\n",
    "total_records = df_bronze.count()\n",
    "print(f\"Read complete: {total_records:,} records\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b84e0",
   "metadata": {},
   "source": [
    "## Type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b96663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting types...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[event_time: string, event_type: string, product_id: string, category_id: string, category_code: string, brand: string, price: string, user_id: string, user_session: string, _row_id: bigint, event_time_raw: string, event_time_typed: timestamp, product_id_raw: string, product_id_typed: bigint, category_id_raw: string, category_id_typed: bigint, price_raw: string, price_typed: double, user_id_raw: string, user_id_typed: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Casting types...\")\n",
    "                                                                                                                        \n",
    "df_bronze_typed = (\n",
    "    df_bronze\n",
    "    .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n",
    "    .withColumn(\"event_time_raw\", col(\"event_time\"))\n",
    "    .withColumn(\n",
    "        \"event_time_typed\",\n",
    "        to_timestamp(\n",
    "            regexp_replace(col(\"event_time\"), \" UTC$\", \"\"),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    .withColumn(\"product_id_raw\", col(\"product_id\"))\n",
    "    .withColumn(\"product_id_typed\", col(\"product_id\").cast(\"long\"))\n",
    "\n",
    "    .withColumn(\"category_id_raw\", col(\"category_id\"))\n",
    "    .withColumn(\"category_id_typed\", col(\"category_id\").cast(\"long\"))\n",
    "\n",
    "    .withColumn(\"price_raw\", col(\"price\"))\n",
    "    .withColumn(\"price_typed\", col(\"price\").cast(\"double\"))\n",
    "\n",
    "    .withColumn(\"user_id_raw\", col(\"user_id\"))\n",
    "    .withColumn(\"user_id_typed\", col(\"user_id\").cast(\"long\"))\n",
    ") \n",
    "df_bronze_typed.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898b85d",
   "metadata": {},
   "source": [
    "## Casting validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf7be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating cast failures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>     (99 + 6) / 110]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Casting validation summary\n",
      "==================================================\n",
      "Total records:      109,950,743\n",
      "Cast failures:   0 (0.0000%)\n",
      "==================================================\n",
      "\n",
      "Elapsed time:     1256.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"Validating cast failures...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_cast_fail = df_bronze_typed.filter(\n",
    "  (col(\"event_time_raw\").isNotNull() & col(\"event_time_typed\").isNull()) |\n",
    "  (col(\"product_id_raw\").isNotNull() & col(\"product_id_typed\").isNull()) |\n",
    "  (col(\"category_id_raw\").isNotNull() & col(\"category_id_typed\").isNull()) |\n",
    "  (col(\"price_raw\").isNotNull() & col(\"price_typed\").isNull()) |\n",
    "  (col(\"user_id_raw\").isNotNull() & col(\"user_id_typed\").isNull())\n",
    ")\n",
    "\n",
    "cast_fail_count = df_cast_fail.count()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "cast_fail_rate = cast_fail_count / total_records if total_records > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Casting validation summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records:      {total_records:,}\")\n",
    "print(f\"Cast failures:   {cast_fail_count:,} ({cast_fail_rate:.4%})\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Elapsed time:     {elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf4666",
   "metadata": {},
   "source": [
    "## Save invalid rows (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c5543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cast failures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cast_fail_count > 0:\n",
    "  print(\"Saving invalid rows to GCS...\")\n",
    "\n",
    "  df_cast_fail_labeled = df_cast_fail.withColumn(\n",
    "      \"failure_reason\",\n",
    "      concat_ws(\", \",\n",
    "          array(\n",
    "              when(col(\"event_time_raw\").isNotNull() & col(\"event_time_typed\").isNull(),\n",
    "                   lit(\"event_time\")).otherwise(lit(\"\")),\n",
    "              when(col(\"product_id_raw\").isNotNull() & col(\"product_id_typed\").isNull(),\n",
    "                   lit(\"product_id\")).otherwise(lit(\"\")),\n",
    "              when(col(\"category_id_raw\").isNotNull() & col(\"category_id_typed\").isNull(),\n",
    "                   lit(\"category_id\")).otherwise(lit(\"\")),\n",
    "              when(col(\"price_raw\").isNotNull() & col(\"price_typed\").isNull(),\n",
    "                   lit(\"price\")).otherwise(lit(\"\")),\n",
    "              when(col(\"user_id_raw\").isNotNull() & col(\"user_id_typed\").isNull(),\n",
    "                   lit(\"user_id\")).otherwise(lit(\"\"))\n",
    "          )\n",
    "      )\n",
    "  )\n",
    "\n",
    "  df_cast_fail_labeled.select(\n",
    "      \"event_time_raw\", \"event_type\", \"product_id_raw\", \"category_id_raw\",\n",
    "      \"category_code\", \"brand\", \"price_raw\", \"user_id_raw\", \"user_session\",\n",
    "      \"failure_reason\"\n",
    "  ).write.mode(\"overwrite\").parquet(OUTPUT_PATHS[\"bronze_invalid\"])\n",
    "\n",
    "  print(\"Invalid rows saved\\n\")\n",
    "else:\n",
    "  print(\"No cast failures\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa734a7",
   "metadata": {},
   "source": [
    "## Build processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781eb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building processed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>      (97 + 4) / 110]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed records: 109,950,743 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Building processed data...\")\n",
    "                                                                                                                        \n",
    "df_processed = (                                                                                                             \n",
    "  df_bronze_typed                                                                                                       \n",
    "  .filter(                                                                                                              \n",
    "      col(\"event_time_typed\").isNotNull() &                                                                                   \n",
    "      col(\"user_id_typed\").isNotNull() &                                                                                      \n",
    "      col(\"product_id_typed\").isNotNull()                                                                                     \n",
    "  )                                                                                                                     \n",
    "    .select(                                                                                           \n",
    "      col(\"event_time_typed\").alias(\"event_time\"),                                                   \n",
    "      col(\"event_type\"),                                                                             \n",
    "      col(\"product_id_typed\").alias(\"product_id\"),                                                   \n",
    "      col(\"category_id_typed\").alias(\"category_id\"),                                                 \n",
    "      col(\"category_code\"),                                                                          \n",
    "      col(\"brand\"),                                                                                  \n",
    "      col(\"price_typed\").alias(\"price\"),                                                             \n",
    "      col(\"user_id_typed\").alias(\"user_id\"),                                                         \n",
    "      col(\"user_session\")                                                                            \n",
    "  )   \n",
    ")    \n",
    "df_processed.cache()                                                                 \n",
    "                                                                                                                        \n",
    "processed_count = df_processed.count()                                                                                          \n",
    "processed_rate = processed_count / total_records if total_records > 0 else 0                                                    \n",
    "                                                                                                                        \n",
    "print(f\"Processed records: {processed_count:,} ({processed_rate:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623ab61",
   "metadata": {},
   "source": [
    "## Write processed Parquet (partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "642adb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved\n"
     ]
    }
   ],
   "source": [
    "df_processed_partitioned = (\n",
    "  df_processed\n",
    "  .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "  .withColumn(\"event_month\", date_format(col(\"event_time\"), \"yyyy-MM\"))\n",
    ")\n",
    "\n",
    "(\n",
    "  df_processed_partitioned.write\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy(\"event_month\", \"event_date\")\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .parquet(OUTPUT_PATHS[\"processed\"])\n",
    ")\n",
    "\n",
    "print(\"Processed data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e54ab3-3e08-4303-969a-6f313cf2c1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
